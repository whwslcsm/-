import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time


class WebsiteCrawler:
    def __init__(self, base_url):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.visited_urls = set()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def is_valid_url(self, url):
        """检查URL是否属于同一域名且格式有效"""
        parsed = urlparse(url)
        return parsed.netloc == self.domain and parsed.scheme in ['http', 'https']

    def get_all_links(self, url):
        """获取页面所有有效链接"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
        except requests.RequestException:
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()

        for tag in soup.find_all('a', href=True):
            full_url = urljoin(url, tag['href'])
            if self.is_valid_url(full_url) and full_url not in self.visited_urls:
                links.add(full_url)

        return links

    def parse_date(self, text):
        """解析日期并返回datetime对象"""
        cleaned_text = text.replace('Published', '').strip()
        try:
            return datetime.strptime(cleaned_text.strip(), '%B %d, %Y')
        except ValueError:
            return None

    def check_date_condition(self, date_str):
        """检查日期是否符合5天内条件"""
        target_date = self.parse_date(date_str)
        if not target_date:
            return False, None

        current_date = datetime.now()
        date_diff = (current_date - target_date).days
        return abs(date_diff) <= 5, target_date.strftime('%B %d, %Y')

    def process_page(self, url):
        """处理单个页面"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
        except requests.RequestException:
            print(f"无法访问: {url}")
            return

        soup = BeautifulSoup(response.text, 'html.parser')
        date_element = soup.find('p', class_='mt-xs')

        if date_element and date_element.text:
            is_within_5days, formatted_date = self.check_date_condition(date_element.text)
            if is_within_5days:
                print(f"[{url}] 5天内发布 ({formatted_date})")
            else:
                print(f"[{url}] 非5天内发布 ({formatted_date})")
        else:
            time.sleep(0.1)
            print(f"[{url}] 未找到发布时间")

    def crawl(self, start_url):
        """开始爬取"""
        queue = [start_url]

        while queue:
            current_url = queue.pop(0)
            if current_url in self.visited_urls:
                continue

            self.visited_urls.add(current_url)
            #print(f"正在处理: {current_url}")

            # 处理当前页面
            self.process_page(current_url)

            # 获取并添加新链接
            new_links = self.get_all_links(current_url)
            queue.extend([link for link in new_links if link not in self.visited_urls])

            # 礼貌性延迟
            time.sleep(1)


if __name__ == "__main__":
    target_url = "https://www.csis.org/"  # 替换为实际目标网站
    crawler = WebsiteCrawler(target_url)
    crawler.crawl(target_url)